{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook im implmenting an image segmentation model on data-science-bowl-2018 data from kaggle\n",
        "## 1. In this notebook im implmenting the network to look at larger details only [applying transposed convolutions]"
      ],
      "metadata": {
        "id": "adJYMzkctNpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqg7y9OAJR9r"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "QicsjUbkJaoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c data-science-bowl-2018\n",
        "! unzip data-science-bowl-2018.zip\n",
        "\n",
        "# Create a folder named 'train_data' and extract stage1_train.zip into it\n",
        "! mkdir train_data\n",
        "! unzip -q stage1_train.zip -d train_data\n"
      ],
      "metadata": {
        "id": "56Gu4mNAJat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * Grab utils file"
      ],
      "metadata": {
        "id": "4GovyEMsSDFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def save_url_content_as_py(url, file_name):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "        with open(file_name, 'w') as file:\n",
        "            file.write(response.text)\n",
        "\n",
        "        print(f\"Content from {url} saved to {file_name} successfully.\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching content from {url}: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "url = 'https://raw.githubusercontent.com/Amr-Abdellatif/Fingerprint-recognition-using-Autoencoders/master/utils.py'  # Replace with your desired URL\n",
        "file_name = \"utils.py\"  # Replace with your desired file name\n",
        "\n",
        "save_url_content_as_py(url, file_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "odp6HrlzXX5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "import imageio.v2\n",
        "from utils import View\n",
        "from utils import train_network"
      ],
      "metadata": {
        "id": "LM6mUPCd_R5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an iterator to go get all path in that directory\n",
        "paths = glob(\"/content/train_data/*\")"
      ],
      "metadata": {
        "id": "bjWJjXqqDHGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(paths))"
      ],
      "metadata": {
        "id": "gKkec3EVD0nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class datasciencebowl(Dataset):\n",
        "    \"\"\"Dataset class for the 2018 Data Science Bowl.\"\"\"\n",
        "    def __init__(self, paths):\n",
        "        \"\"\"paths: a list of paths to every image folder in the dataset\"\"\"\n",
        "        self.paths = paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #There is only one image in each images path. So we will grab the \"first\" thing we find with \"[0]\" at the end\n",
        "        img_path = glob(self.paths[idx] + \"/images/*\")[0]\n",
        "        #but there are multiple mask images in each mask path\n",
        "        mask_imgs = glob(self.paths[idx] + \"/masks/*\")\n",
        "        #the image shape is (W, H, 4), the last dimension is an 'alpha' channel that is not used\n",
        "        img = imageio.v2.imread(img_path)[:,:,0:3]#trim off the alpha so we get (W, H, 3)\n",
        "        #Now we want this as (3, W, H), which is the normal shape for PyTorch\n",
        "        img = np.moveaxis(img, -1, 0)\n",
        "        #Last step for the image, re-scale it to the range [0, 1]\n",
        "        img = img/255.0\n",
        "\n",
        "        #Every mask image is going to have a shape of (W, H) which has a value of 1 if the pixel is of a nuclei, and a value of 0 if the image is background/ a  _different_ nuclei\n",
        "        masks = [imageio.v2.imread(f)/255.0 for f in mask_imgs]\n",
        "\n",
        "        #Since we want to do simple segmentation, we will create one final mask that contains _all_ nuclei pixels from _every_ mask\n",
        "        final_mask = np.zeros(masks[0].shape)\n",
        "        for m in masks:\n",
        "            final_mask = np.logical_or(final_mask, m)\n",
        "        final_mask = final_mask.astype(np.float32)\n",
        "\n",
        "        #Not every image in the dataset is the same size.  To simplify the problem, we are going to re-size  every image to be (256, 256)\n",
        "        img, final_mask = torch.tensor(img), torch.tensor(final_mask).unsqueeze(0) #First we convert to PyTorch tensors\n",
        "        #The interpolate function can be used to re-size a batch of images. So we make each image a \"batch\" of 1\n",
        "        img = F.interpolate(img.unsqueeze(0), (256, 256))\n",
        "        final_mask = F.interpolate(final_mask.unsqueeze(0), (244, 244))\n",
        "        #Now the shapes  are (B=1, C, W, H) We need to convert them back to FloatTensors and grab the first item in the \"batch\". This will return a tuple of: (3, 256, 256), (1, 256, 256)\n",
        "        return img.type(torch.FloatTensor)[0], final_mask.type(torch.FloatTensor)[0]\n",
        "#Caption: Class for the 2018 data science bowl dataset.  Each image has a corresponding folder of masks, one for every object in the imag"
      ],
      "metadata": {
        "id": "A7dWbkLXJay3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the Dataset class object\n",
        "dsb_data = datasciencebowl(paths)\n",
        "\n",
        "plt.figure(figsize=(16,10))\n",
        "#Plot the original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(dsb_data[0][0].permute(1,2,0).numpy())\n",
        "#Plot the mask\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(dsb_data[0][1].numpy()[0,:], cmap='gray')\n",
        "\n"
      ],
      "metadata": {
        "id": "do0-nhD2Ja4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(dsb_data[2][0].permute(1,2,0).numpy())\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(dsb_data[2][1].numpy()[0,:], cmap='gray')\n",
        "print(dsb_data[2][1].shape) # look at the shape of the image\n"
      ],
      "metadata": {
        "id": "p03u9r-QJa-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dsb_data)"
      ],
      "metadata": {
        "id": "0GawgcvtUnqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split , test_split = torch.utils.data.random_split(dsb_data,[500,len(dsb_data)-500])"
      ],
      "metadata": {
        "id": "u-TKOe83JbDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_split) , len(test_split)"
      ],
      "metadata": {
        "id": "2s-YL7WqJbI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split[0][0].shape , test_split[0][0].shape"
      ],
      "metadata": {
        "id": "vN0PMUKv8a1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify hyperparameters\n",
        "C = 3 # since original images are 3 channels RGB\n",
        "n_filters = 32 # the output filters\n",
        "loss_function = nn.BCEWithLogitsLoss() # binarycross entropy because in every image we have two classes [nuclei / background ]"
      ],
      "metadata": {
        "id": "UKi5pSg9JbTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnnlayer(in_filters,out_filters,kernel_size=3):\n",
        "  \"\"\"\n",
        "  in_filters: how many channels are in the input to this layer\n",
        "  out_filters: how many channels should this layer output\n",
        "  kernel_size: how large should the filters of this layer be\n",
        "  \"\"\"\n",
        "  padding = kernel_size //2\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_filters,out_filters,kernel_size),\n",
        "      nn.BatchNorm2d(out_filters),\n",
        "      nn.LeakyReLU()\n",
        "    )"
      ],
      "metadata": {
        "id": "LFnRTMNYU_ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_model = nn.Sequential(\n",
        "    cnnlayer(C,n_filters),\n",
        "    cnnlayer(n_filters,n_filters),\n",
        "    nn.MaxPool2d(2),\n",
        "    cnnlayer(n_filters,2*n_filters),\n",
        "    cnnlayer(2*n_filters,2*n_filters),\n",
        "    cnnlayer(2*n_filters,2*n_filters),\n",
        "    nn.ConvTranspose2d(2*n_filters,n_filters,(3,3),padding=1,output_padding=1,stride=2),\n",
        "    nn.BatchNorm2d(n_filters),\n",
        "    nn.LeakyReLU(),\n",
        "    cnnlayer(n_filters,n_filters),\n",
        "    nn.Conv2d(n_filters,1,(3,3),padding=1)\n",
        ")"
      ],
      "metadata": {
        "id": "O0BD6uzavxJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seg_results = train_network(segmentation_model,loss_func,train_seg_loader, epochs=10, device=device, val_loader=test_seg_loader)"
      ],
      "metadata": {
        "id": "oHcUFfdJvxPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 6\n",
        "with torch.inference_mode():\n",
        "  logits = segmentation_model(test_split[index][0]. unsqueeze(0).to(device))[0].cpu()\n",
        "  pred = torch.sigmoid(logits) >= 0.5\n",
        "plt.figure(figsize=(16,10))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(test_split[index][0].permute(1,2,0).numpy(),cmap='gray') # the original image\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(test_split[index][1].numpy()[0,:], cmap='gray') # plot test image\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(pred.numpy()[0,:], cmap='gray') # plot the prediction with threshold 0.5"
      ],
      "metadata": {
        "id": "eL_czg66vxVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSHyV3aevxac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}