{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vgtoWDAAEkVd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "C46_fSu-GpTi"
      },
      "outputs": [],
      "source": [
        "image = torch.randn((1,1,572,572))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "u2S4tjWsHWvf"
      },
      "outputs": [],
      "source": [
        "def double_convs(in_channels,out_channels,kernel_size=3):\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_channels,out_channels,kernel_size),\n",
        "      nn.ReLU(inplace=True), # inplace=True This means that the output tensor is written to the same memory location as the input tensor -> more memory efficient\n",
        "      nn.Conv2d(out_channels,out_channels,kernel_size),\n",
        "      nn.ReLU(inplace=True),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrI6SLwGHrcs",
        "outputId": "e1a0fe40-e429-496c-b5f4-7fb6db0d1456"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 568, 568])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_instance = double_convs(1,64)\n",
        "pass_image = model_instance(image)\n",
        "pass_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zSXuW6dqNoDT"
      },
      "outputs": [],
      "source": [
        "def crop_image(tensor , target_tensor):\n",
        "  target_size = target_tensor.size()[2] # image.shape[-2]\n",
        "  tensor_size = tensor.size()[2]\n",
        "  delta = tensor_size - target_size # (572 - 262) = 10\n",
        "  delta = delta //2  # answer is (5) cause we want to crop from left and right\n",
        "  # delta : tensor_size - delta   --> 5 : 572 - 5 --> (5 : 567)\n",
        "  return tensor [ : , : , delta : tensor_size - delta , delta : tensor_size - delta ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "0OYOKFKbIfvz"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet,self).__init__()\n",
        "\n",
        "    # Building Blocks\n",
        "\n",
        "    self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "    self.down_conv_1 = double_convs(1,64)\n",
        "    self.down_conv_2 = double_convs(64,128)\n",
        "    self.down_conv_3 = double_convs(128,256)\n",
        "    self.down_conv_4 = double_convs(256,512)\n",
        "    self.down_conv_5 = double_convs(512,1024)\n",
        "\n",
        "    self.up_trans_1 = nn.ConvTranspose2d(in_channels=1024,\n",
        "                                         out_channels=512,\n",
        "                                         kernel_size=2,\n",
        "                                         stride=2)\n",
        "\n",
        "    self.up_conv_1 = double_convs(1024,512)\n",
        "\n",
        "    self.up_trans_2 = nn.ConvTranspose2d(in_channels=512,\n",
        "                                         out_channels=256,\n",
        "                                         kernel_size=2,\n",
        "                                         stride=2)\n",
        "\n",
        "    self.up_conv_2 = double_convs(512,256)\n",
        "\n",
        "    self.up_trans_3 = nn.ConvTranspose2d(in_channels=256,\n",
        "                                         out_channels=128,\n",
        "                                         kernel_size=2,\n",
        "                                         stride=2)\n",
        "\n",
        "    self.up_conv_3 = double_convs(256,128)\n",
        "\n",
        "    self.up_trans_4 = nn.ConvTranspose2d(in_channels=128,\n",
        "                                         out_channels=64,\n",
        "                                         kernel_size=2,\n",
        "                                         stride=2)\n",
        "\n",
        "    self.up_conv_4 = double_convs(128,64)\n",
        "\n",
        "    self.out = nn.Conv2d(64,2,kernel_size=1)\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    # Encoder part .. first half\n",
        "    X1 = self.down_conv_1(x) # ---> x here is like the image we're passing into the model\n",
        "    X2 = self.max_pool_2x2(X1) # maxpool then double convs\n",
        "    X3 = self.down_conv_2(X2) # ---> double convs (64,128)\n",
        "    X4 = self.max_pool_2x2(X3) # maxpool then double convs\n",
        "    X5 = self.down_conv_3(X4)  # ---> double convs (128,256)\n",
        "    X6 = self.max_pool_2x2(X5) # maxpool then double convs\n",
        "    X7 = self.down_conv_4(X6) # ---> double convs (256,512)\n",
        "    X8 = self.max_pool_2x2(X7) # maxpool then double convs\n",
        "    X9 = self.down_conv_5(X8) # double convs (512,1024)\n",
        "\n",
        "    print(f\"The shape of the first Encoder part{X9.shape}\") # check if it matches the paper\n",
        "\n",
        "    # Decoder part .. first half\n",
        "\n",
        "    X10 = self.up_trans_1(X9)\n",
        "    # print(X10.shape)\n",
        "    y = crop_image(X7,X10) # tensor swapping is bad here\n",
        "    print(y.shape)\n",
        "    X11 = self.up_conv_1(torch.concat((X10,y),1))\n",
        "    print(X11.shape)\n",
        "\n",
        "    X12 = self.up_trans_2(X11)\n",
        "    y = crop_image(X5,X12) # tensor swapping is bad here\n",
        "    X13 = self.up_conv_2(torch.concat((X12,y),1))\n",
        "    print(X13.shape)\n",
        "\n",
        "    X14 = self.up_trans_3(X13)\n",
        "    y = crop_image(X3,X14) # tensor swapping is bad here\n",
        "    X15 = self.up_conv_3(torch.concat((X14,y),1))\n",
        "    print(X15.shape)\n",
        "\n",
        "    X16 = self.up_trans_4(X15)\n",
        "    y = crop_image(X1,X16) # tensor swapping is bad here\n",
        "    X17 = self.up_conv_4(torch.concat((X16,y),1))\n",
        "    print(X17.shape)\n",
        "\n",
        "    X18 = self.out(X17)\n",
        "    print(X18.shape)\n",
        "    return X18\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9xHHiLFIf51",
        "outputId": "32a66d34-f631-4ae4-fba7-30fb0b0137a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the first Encoder parttorch.Size([1, 1024, 28, 28])\n",
            "torch.Size([1, 512, 56, 56])\n",
            "torch.Size([1, 512, 52, 52])\n",
            "torch.Size([1, 256, 100, 100])\n",
            "torch.Size([1, 128, 196, 196])\n",
            "torch.Size([1, 64, 388, 388])\n",
            "torch.Size([1, 2, 388, 388])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[[-0.0569, -0.0674, -0.0583,  ..., -0.0594, -0.0571, -0.0603],\n",
              "          [-0.0565, -0.0660, -0.0585,  ..., -0.0562, -0.0554, -0.0630],\n",
              "          [-0.0544, -0.0633, -0.0549,  ..., -0.0714, -0.0559, -0.0638],\n",
              "          ...,\n",
              "          [-0.0544, -0.0631, -0.0604,  ..., -0.0684, -0.0556, -0.0586],\n",
              "          [-0.0641, -0.0624, -0.0514,  ..., -0.0622, -0.0572, -0.0575],\n",
              "          [-0.0550, -0.0554, -0.0590,  ..., -0.0520, -0.0639, -0.0597]],\n",
              "\n",
              "         [[-0.0008,  0.0084, -0.0034,  ..., -0.0079, -0.0073, -0.0103],\n",
              "          [ 0.0006, -0.0114, -0.0009,  ..., -0.0126, -0.0146, -0.0061],\n",
              "          [-0.0042,  0.0017, -0.0089,  ...,  0.0070, -0.0154, -0.0119],\n",
              "          ...,\n",
              "          [-0.0045, -0.0038, -0.0161,  ..., -0.0076, -0.0054, -0.0118],\n",
              "          [-0.0038, -0.0103, -0.0012,  ..., -0.0124, -0.0190, -0.0150],\n",
              "          [-0.0069, -0.0110, -0.0040,  ..., -0.0132, -0.0059, -0.0089]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1 = UNet()\n",
        "model_1(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aos610H_IgBQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZzC-aToIgIU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8d9UTPrIgOn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEEeJq8aIgU7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
